{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "81a23826",
   "metadata": {},
   "source": [
    "# Example of using Pinecode as embeddings DB for Neural Search of Youtube videos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d81d2af5",
   "metadata": {},
   "source": [
    "## Load libraries and setup 3rd party applications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7799aa4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-22 11:11:01.100287: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-01-22 11:11:01.757869: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-01-22 11:11:01.758001: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-01-22 11:11:01.758010: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "from googleapiclient.discovery import build # pip install --upgrade google-api-python-client\n",
    "from pathlib import Path\n",
    "import json\n",
    "import requests\n",
    "import urllib\n",
    "from os.path import exists\n",
    "import os\n",
    "\n",
    "# !sudo apt update -y && sudo apt install ffmpeg -y\n",
    "#!pip install --upgrade protobuf\n",
    "#!pip install --upgrade tensorflow\n",
    "#!pip uninstall keras\n",
    "#!pip install --upgrade keras\n",
    "from pytube import YouTube  # !pip install pytube\n",
    "from pytube.exceptions import RegexMatchError\n",
    "import scrapetube\n",
    "import youtube_dl\n",
    "\n",
    "from tqdm.auto import tqdm  # !pip install tqdm\n",
    "\n",
    "import whisper # !pip install git+https://github.com/openai/whisper.git\n",
    "import torch  # pytorch install steps: pytorch.org\n",
    "\n",
    "import pinecone # pip install --upgrade pinecone-client\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab669815",
   "metadata": {},
   "source": [
    "## Define Inputs\n",
    "channel_names is a list of youtube channel names to index<br>\n",
    "pinecone_api_key and google_api_key requires free registration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4d32377b",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_dir = './audio'\n",
    "channel_names = ['@promptmuse']\n",
    "pinecone_api_key = 'c65fa925-08e1-4af0-b08b-1104c6ffba25' # https://app.pinecone.io/projects\n",
    "google_api_key = 'AIzaSyAIIY6OsTws8dTfoyxNmJLmnfmH2f859Fw' # https://console.cloud.google.com/apis/dashboard\n",
    "\n",
    "youtube_dl_options = {\n",
    "    'skip_download': True,\n",
    "    'ignoreerrors': True\n",
    "}\n",
    "\n",
    "# name of Pinecone index to use\n",
    "index_id = \"audio\"\n",
    "# we encode and insert in batches of 64\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9780940e",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f4ed9341",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_video_in_channel(channel_id):\n",
    "    video_IDs = []\n",
    "    video_titles = {}\n",
    "    videos = scrapetube.get_channel(channel_id)\n",
    "\n",
    "    try:\n",
    "        for video in videos:\n",
    "            video_IDs.append(video['videoId'])\n",
    "            video_titles[video['videoId']] = video['title']['runs'][0]['text']\n",
    "    except:\n",
    "        with youtube_dl.YoutubeDL(youtube_dl_options) as ydl:\n",
    "            videos = ydl.extract_info(f'https://www.youtube.com/user/{channel_id}/videos')\n",
    "\n",
    "    if videos == None:\n",
    "        with youtube_dl.YoutubeDL(youtube_dl_options) as ydl:\n",
    "            videos = ydl.extract_info(f'https://www.youtube.com/{channel_id}/videos')        \n",
    "                \n",
    "    if videos != None:    \n",
    "        for item in videos['entries']:\n",
    "            video_IDs.append(item['id'])\n",
    "            video_titles[item['id']] = item['title']\n",
    "            \n",
    "    if len(video_IDs) == 0:\n",
    "        with youtube_dl.YoutubeDL(youtube_dl_options) as ydl:\n",
    "            videos = ydl.extract_info(f'https://www.youtube.com/channel/{channel_id}')\n",
    "        for item in videos['entries']:\n",
    "            video_IDs.append(item['id'])\n",
    "            video_titles[item['id']] = item['title']\n",
    "        \n",
    "    return video_IDs, video_titles\n",
    "        \n",
    "def get_youtube_channel_id(channel_name):\n",
    "    try:\n",
    "        youtube = build('youtube', 'v3', developerKey=google_api_key)\n",
    "        channels_response = youtube.channels().list(\n",
    "                forUsername=channel_name,\n",
    "                part=\"id, snippet, statistics, contentDetails, topicDetails\"\n",
    "        ).execute()\n",
    "        response = channels_response['items'][0]['id']\n",
    "        return response\n",
    "    except:\n",
    "        return channel_name\n",
    "\n",
    "def save_audio_from_videoIDs(save_path, video_IDs):\n",
    "    for videoID in tqdm(video_IDs):\n",
    "        check_file = f\"{save_path}/{videoID}.mp3\"\n",
    "        if exists(check_file):\n",
    "            continue\n",
    "\n",
    "        # url of video to be downloaded\n",
    "        url = f\"https://youtu.be/{videoID}\"\n",
    "\n",
    "        # try to create a YouTube vid object\n",
    "        try:\n",
    "            yt = YouTube(url)\n",
    "        except RegexMatchError:\n",
    "            print(f\"RegexMatchError for '{url}'\")\n",
    "            continue\n",
    "\n",
    "        itag = None\n",
    "        # we only want audio files\n",
    "        files = yt.streams.filter(only_audio=True)\n",
    "        for file in files:\n",
    "            # and of those audio files we grab the first audio for mp4 (eg mp3)\n",
    "            if file.mime_type == 'audio/mp4':\n",
    "                itag = file.itag\n",
    "                break\n",
    "        if itag is None:\n",
    "            # just incase no MP3 audio is found (shouldn't happen)\n",
    "            print(\"NO MP3 AUDIO FOUND\")\n",
    "            continue\n",
    "\n",
    "        # get the correct mp3 'stream'\n",
    "        stream = yt.streams.get_by_itag(itag)\n",
    "        # downloading the audio\n",
    "        try:\n",
    "            # only download mp3 if it does not exist\n",
    "            stream.download(output_path=save_path, filename=f\"{videoID}.mp3\")\n",
    "        except:\n",
    "            print(f\"error downloading audio for video ID {videoID}\")\n",
    "            \n",
    "def get_text_from_data(start, end, data):\n",
    "    text = \"\"\n",
    "    for i in range(start,end):\n",
    "        text += data[i]['text']+' '\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e199bc60",
   "metadata": {},
   "source": [
    "## Initialize Whisper Model for transcription and Pinecone index\n",
    "Use GPU if available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d51c2270",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'dimension': 768,\n",
       " 'index_fullness': 0.4,\n",
       " 'namespaces': {'': {'vector_count': 805171}},\n",
       " 'total_vector_count': 805171}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)\n",
    "\n",
    "model = whisper.load_model(\"small\").to(device)\n",
    "\n",
    "model_id = \"multi-qa-mpnet-base-dot-v1\"\n",
    "model_embed = SentenceTransformer(model_id)\n",
    "dim = model_embed.get_sentence_embedding_dimension()\n",
    "\n",
    "pinecone.init(\n",
    "    api_key=pinecone_api_key,  # app.pinecone.io\n",
    "    environment=\"us-west1-gcp\"\n",
    ")\n",
    "if index_id not in pinecone.list_indexes():\n",
    "    pinecone.create_index(\n",
    "        index_id,\n",
    "        dim,\n",
    "        metric=\"dotproduct\"\n",
    "    )\n",
    "\n",
    "index = pinecone.Index(index_id)\n",
    "index.describe_index_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "640e798e",
   "metadata": {},
   "source": [
    "## Create embeddings from youtube channel\n",
    "This takes a while to run as is it downloads and transcribes every video in the youtube channel<br>\n",
    "If videos do not download, try alternative methods as youtube channels are not setup consistently"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62c08b0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@promptmuse\n",
      "@promptmuse\n",
      "[youtube:tab] @promptmuse: Downloading webpage\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Unable to download webpage: HTTP Error 404: Not Found (caused by <HTTPError 404: 'Not Found'>); please report this issue on https://yt-dl.org/bug . Make sure you are using the latest version; see  https://yt-dl.org/update  on how to update. Be sure to call youtube-dl with the --verbose flag and include its complete output.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[youtube:tab] @promptmuse: Downloading webpage\n",
      "[download] Downloading playlist: Prompt Muse - Videos\n",
      "[youtube:tab] playlist Prompt Muse - Videos: Downloading 19 videos\n",
      "[download] Downloading video 1 of 19\n",
      "[youtube] FKoy7bncHLs: Downloading webpage\n",
      "[download] Downloading video 2 of 19\n",
      "[youtube] RI6ZLY2o900: Downloading webpage\n",
      "[download] Downloading video 3 of 19\n",
      "[youtube] 3wQBsFftbv8: Downloading webpage\n",
      "[download] Downloading video 4 of 19\n",
      "[youtube] XjObqq6we4U: Downloading webpage\n",
      "[download] Downloading video 5 of 19\n",
      "[youtube] xdE_h-xjMPs: Downloading webpage\n",
      "[download] Downloading video 6 of 19\n",
      "[youtube] 6ssyJx5CqjI: Downloading webpage\n",
      "[download] Downloading video 7 of 19\n",
      "[youtube] goRvGFs1sdc: Downloading webpage\n",
      "[download] Downloading video 8 of 19\n",
      "[youtube] Dq04W_sVZyk: Downloading webpage\n",
      "[youtube] Dq04W_sVZyk: Downloading MPD manifest\n",
      "[download] Downloading video 9 of 19\n",
      "[youtube] FLA6KCm4zLw: Downloading webpage\n",
      "[youtube] FLA6KCm4zLw: Downloading MPD manifest\n",
      "[download] Downloading video 10 of 19\n",
      "[youtube] jo--DkGakAI: Downloading webpage\n",
      "[youtube] jo--DkGakAI: Downloading MPD manifest\n",
      "[download] Downloading video 11 of 19\n",
      "[youtube] Geh2tk_4gc0: Downloading webpage\n",
      "[youtube] Geh2tk_4gc0: Downloading MPD manifest\n",
      "[download] Downloading video 12 of 19\n",
      "[youtube] mTpS59jTBRs: Downloading webpage\n",
      "[youtube] mTpS59jTBRs: Downloading MPD manifest\n",
      "[download] Downloading video 13 of 19\n",
      "[youtube] caBGryJg5KM: Downloading webpage\n",
      "[download] Downloading video 14 of 19\n",
      "[youtube] IAzobjisCKQ: Downloading webpage\n",
      "[youtube] IAzobjisCKQ: Downloading MPD manifest\n",
      "[download] Downloading video 15 of 19\n",
      "[youtube] FYXGXS6WFCQ: Downloading webpage\n",
      "[youtube] FYXGXS6WFCQ: Downloading MPD manifest\n",
      "[download] Downloading video 16 of 19\n",
      "[youtube] BVai4eT3xXQ: Downloading webpage\n",
      "[download] Downloading video 17 of 19\n",
      "[youtube] QKhLW02euIA: Downloading webpage\n",
      "[youtube] QKhLW02euIA: Downloading MPD manifest\n",
      "[download] Downloading video 18 of 19\n",
      "[youtube] 27l_TFWm6jM: Downloading webpage\n",
      "[youtube] 27l_TFWm6jM: Downloading MPD manifest\n",
      "[download] Downloading video 19 of 19\n",
      "[youtube] qcxps9tVM9M: Downloading webpage\n",
      "[youtube] qcxps9tVM9M: Downloading MPD manifest\n",
      "[download] Finished downloading playlist: Prompt Muse - Videos\n",
      "['FKoy7bncHLs', 'RI6ZLY2o900', '3wQBsFftbv8', 'XjObqq6we4U', 'xdE_h-xjMPs', '6ssyJx5CqjI', 'goRvGFs1sdc', 'Dq04W_sVZyk', 'FLA6KCm4zLw', 'jo--DkGakAI', 'Geh2tk_4gc0', 'mTpS59jTBRs', 'caBGryJg5KM', 'IAzobjisCKQ', 'FYXGXS6WFCQ', 'BVai4eT3xXQ', 'QKhLW02euIA', '27l_TFWm6jM', 'qcxps9tVM9M']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13c5feea400c42a99ea4b98f3008ac47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6d3df596d424856876582a5ccdb3c35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "audio/xdE_h-xjMPs.mp3\n"
     ]
    }
   ],
   "source": [
    "for channel_name in channel_names:\n",
    "    try:\n",
    "        print(channel_name)\n",
    "        channel_id = get_youtube_channel_id(channel_name)\n",
    "        print(channel_id)\n",
    "        video_IDs, video_titles = get_all_video_in_channel(channel_id)\n",
    "        print(video_IDs)\n",
    "        save_audio_from_videoIDs(audio_dir, video_IDs)\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "    # get list of MP3 audio files\n",
    "    paths = [str(x) for x in Path(audio_dir).glob('*.mp3')]\n",
    "    \n",
    "    transcriptions = []\n",
    "    for i, path in enumerate(tqdm(paths)):\n",
    "        _id = path.split('/')[-1][:-4]\n",
    "        # transcribe to get speech-to-text data\n",
    "        print(path)\n",
    "        try:\n",
    "            result = model.transcribe(path)\n",
    "        except:\n",
    "            print('error, removing file ',path)\n",
    "            os.remove(path)\n",
    "            continue\n",
    "        # add results to data list\n",
    "        transcriptions.extend(result['segments'])\n",
    "        \n",
    "        # set window (length of text chunk) and stride\n",
    "        window = 1\n",
    "        stride = 1  # smaller stride creates overlap\n",
    "        \n",
    "        data = []\n",
    "        results = []\n",
    "    \n",
    "        with open(\"transcription.jsonl\", \"w\", encoding=\"utf-8\") as fp:\n",
    "            _id = path.split('/')[-1][:-4]\n",
    "            # transcribe to get speech-to-text data\n",
    "            result = model.transcribe(path)\n",
    "            segments = result['segments']\n",
    "            for j in range(0, len(segments), stride):\n",
    "                j_end = min(j+window, len(segments)-1)\n",
    "                text = ''.join([x[\"text\"] for x in segments[j:j_end]])\n",
    "                start = segments[j]['start']\n",
    "                end = segments[j_end]['end']\n",
    "                row_id = f\"{_id}-t{segments[j]['start']}\"\n",
    "                meta = {\n",
    "                    **{\n",
    "                        \"id\": row_id,\n",
    "                        \"text\": text.strip(),\n",
    "                        \"start\": start,\n",
    "                        \"end\": end,\n",
    "                        \"url\": f\"https://youtu.be/{_id}\",\n",
    "                        \"name\":_id,\n",
    "                        \"title\":video_titles[_id]\n",
    "                    }\n",
    "                }\n",
    "                data.append(meta)\n",
    "                json.dump(meta, fp)\n",
    "                fp.write('\\n')\n",
    "                    \n",
    "        new_data = []\n",
    "        \n",
    "        window = 6  # number of sentences to combine\n",
    "        stride = 3  # number of sentences to 'stride' over, used to create overlap\n",
    "        \n",
    "        for i in tqdm(range(0, len(data), stride)):\n",
    "            i_end = min(len(data)-1, i+window)\n",
    "            if data[i]['name'] != data[i_end]['name']:\n",
    "                # in this case we skip this entry as we have start/end of two videos\n",
    "                continue\n",
    "            text = get_text_from_data(i, i_end, data)\n",
    "            new_data.append({\n",
    "                'start': data[i]['start'],\n",
    "                'end': data[i_end]['end'],\n",
    "                'text': text,\n",
    "                'id': data[i]['id'],\n",
    "                'url': data[i]['url']+'?t='+str(int(data[i]['start'])),\n",
    "                \"name\":data[i]['name'],\n",
    "                \"title\":data[i]['title'],\n",
    "            })\n",
    "            \n",
    "        # loop through in batches of 64\n",
    "        index = pinecone.Index(index_id)\n",
    "        for j in tqdm(range(0, len(new_data), batch_size)):\n",
    "            # find end position of batch (for when we hit end of data)\n",
    "            j_end = min(len(new_data)-1, j+batch_size)\n",
    "            # extract the metadata like text, start/end positions, etc\n",
    "            batch_meta = [{\n",
    "                \"text\": new_data[x][\"text\"],\n",
    "                \"start\": new_data[x][\"start\"],\n",
    "                \"end\": new_data[x][\"end\"],\n",
    "                \"url\": new_data[x][\"url\"],\n",
    "                \"name\": new_data[x][\"name\"],\n",
    "                \"title\": new_data[x][\"title\"]\n",
    "            } for x in range(j, j_end)]\n",
    "            # extract only text to be encoded by embedding model\n",
    "            batch_text = [row['text'] for row in new_data[j:j_end]]\n",
    "            # create the embedding vectors\n",
    "            batch_embeds = model_embed.encode(batch_text).tolist()\n",
    "            # extract IDs to be attached to each embedding and metadata\n",
    "            batch_ids = [row['id'] for row in new_data[j:j_end]]\n",
    "            # 'upsert' (eg insert) IDs, embeddings, and metadata to index\n",
    "            try:\n",
    "                to_upsert = list(zip(batch_ids, batch_embeds, batch_meta))\n",
    "                index.upsert(to_upsert)\n",
    "            except:\n",
    "                continue\n",
    "        print('removing file ',path)\n",
    "        os.remove(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eca2c58",
   "metadata": {},
   "source": [
    "## Query pinecone index for answer to question with video link in URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b7c92a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the query or question to ask\n",
    "query = \"what is OpenAI's CLIP?\"\n",
    "# Create and embedding representing the question\n",
    "xq = model_embed.encode(query).tolist()\n",
    "# Search the index for the top (k) answers \n",
    "results = index.query(xq, top_k=5, include_metadata=True)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8801530b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
